{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow_probability==0.16.0 in /usr/local/lib/python3.8/dist-packages (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (0.5.3)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow_probability==0.16.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Single-level algorithm using the Milstein scheme\n",
    "# For more detailed explanations of the training and model parameters\n",
    "# see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
    "\n",
    "# Uncomment this line after starting the container\n",
    "!pip install -U tensorflow_probability==0.16.0\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.compat.v1.keras import initializers\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
    "def neural_net(\n",
    "    x, xi_approx, neurons, is_training, name, mv_decay=0.9, dtype=tf.float32\n",
    "):\n",
    "    def approx_test():\n",
    "        return xi_approx\n",
    "\n",
    "    def approx_learn():\n",
    "        return x\n",
    "\n",
    "    x = tf.cond(pred=is_training, true_fn=approx_learn, false_fn=approx_test)\n",
    "\n",
    "    def _batch_normalization(_x):\n",
    "        beta = tf.compat.v1.get_variable(\n",
    "            \"beta\", [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer()\n",
    "        )\n",
    "        gamma = tf.compat.v1.get_variable(\n",
    "            \"gamma\", [_x.get_shape()[-1]], dtype, init_ops.ones_initializer()\n",
    "        )\n",
    "        mv_mean = tf.compat.v1.get_variable(\n",
    "            \"mv_mean\",\n",
    "            [_x.get_shape()[-1]],\n",
    "            dtype,\n",
    "            init_ops.zeros_initializer(),\n",
    "            trainable=False,\n",
    "        )\n",
    "        mv_variance = tf.compat.v1.get_variable(\n",
    "            \"mv_variance\",\n",
    "            [_x.get_shape()[-1]],\n",
    "            dtype,\n",
    "            init_ops.ones_initializer(),\n",
    "            trainable=False,\n",
    "        )\n",
    "        mean, variance = tf.nn.moments(x=_x, axes=[0], name=\"moments\")\n",
    "        tf.compat.v1.add_to_collection(\n",
    "            tf.compat.v1.GraphKeys.UPDATE_OPS,\n",
    "            assign_moving_average(mv_mean, mean, mv_decay, True),\n",
    "        )\n",
    "        tf.compat.v1.add_to_collection(\n",
    "            tf.compat.v1.GraphKeys.UPDATE_OPS,\n",
    "            assign_moving_average(mv_variance, variance, mv_decay, False),\n",
    "        )\n",
    "        mean, variance = tf.cond(\n",
    "            pred=is_training, true_fn=lambda: (mean, variance), false_fn=lambda: (mv_mean, mv_variance)\n",
    "        )\n",
    "        return tf.nn.batch_normalization(_x, mean, variance, beta, gamma, 1e-6)\n",
    "\n",
    "    def _layer(_x, out_size, activation_fn):\n",
    "        w = tf.compat.v1.get_variable(\n",
    "            \"weights\",\n",
    "            [_x.get_shape().as_list()[-1], out_size],\n",
    "            dtype,\n",
    "            initializers.glorot_normal(),\n",
    "        )\n",
    "        return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
    "\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        x = _batch_normalization(x)\n",
    "        for i in range(len(neurons)):\n",
    "            with tf.compat.v1.variable_scope(\"layer_%i_\" % (i + 1)):\n",
    "                x = _layer(\n",
    "                    x, neurons[i], tf.nn.tanh if i < len(neurons) - 1 else tf.identity\n",
    "                )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
    "# Minor adjustments to file output and in lines 108-115 changed to exponential decay\n",
    "\n",
    "\n",
    "def train_and_test(\n",
    "    xi,\n",
    "    xi_approx,\n",
    "    x_sde,\n",
    "    phi,\n",
    "    u_reference,\n",
    "    neurons,\n",
    "    train_steps,\n",
    "    mc_rounds,\n",
    "    mc_freq,\n",
    "    file_name,\n",
    "    dtype=tf.float32,\n",
    "):\n",
    "    def _approximate_errors():\n",
    "        lr, gs = sess.run([learning_rate, global_step])\n",
    "        l1_err, l2_err, li_err = 0.0, 0.0, 0.0\n",
    "        rel_l1_err, rel_l2_err, rel_li_err = 0.0, 0.0, 0.0\n",
    "        for _ in range(mc_rounds):\n",
    "            (\n",
    "                plot_xi,\n",
    "                plot_approx,\n",
    "                plot_ref,\n",
    "                l1,\n",
    "                l2,\n",
    "                li,\n",
    "                rl1,\n",
    "                rl2,\n",
    "                rli,\n",
    "                appr,\n",
    "                ref,\n",
    "            ) = sess.run(\n",
    "                [\n",
    "                    xi_approx,\n",
    "                    u_approx,\n",
    "                    u_reference,\n",
    "                    err_l_1,\n",
    "                    err_l_2,\n",
    "                    err_l_inf,\n",
    "                    rel_err_l_1,\n",
    "                    rel_err_l_2,\n",
    "                    rel_err_l_inf,\n",
    "                    approx,\n",
    "                    reference,\n",
    "                ],\n",
    "                feed_dict={is_training: False},\n",
    "            )\n",
    "            l1_err, l2_err, li_err = (l1_err + l1, l2_err + l2, np.maximum(li_err, li))\n",
    "            rel_l1_err, rel_l2_err, rel_li_err = (\n",
    "                rel_l1_err + rl1,\n",
    "                rel_l2_err + rl2,\n",
    "                np.maximum(rel_li_err, rli),\n",
    "            )\n",
    "        l1_err, l2_err = l1_err / mc_rounds, np.sqrt(l2_err / mc_rounds)\n",
    "        rel_l1_err, rel_l2_err = rel_l1_err / mc_rounds, np.sqrt(rel_l2_err / mc_rounds)\n",
    "        t_mc = time.time()\n",
    "\n",
    "        file_out.write(\n",
    "            \"%i, %f, %f, %f, %f \\n\"\n",
    "            % (gs, li_err, lr, t1_train - t0_train, t_mc - t1_train)\n",
    "        )\n",
    "        file_out.flush()\n",
    "        \n",
    "\n",
    "def phi(x, sigma, mu, T, K, axis=1):\n",
    "    payoffcoarse = tf.exp(-mu * T) * tf.maximum(x - K, 0.0)\n",
    "    return payoffcoarse\n",
    "\n",
    "# Milstein scheme\n",
    "def sde_body(idx, s, sigma, mu, T, K, samples, batchsize):\n",
    "    h = T / N\n",
    "    z = tf.random.normal(shape=(samples, batch_size, 1), stddev=1.0, dtype=dtype)\n",
    "    s = (\n",
    "        s\n",
    "        + mu * s * h\n",
    "        + sigma * s * tf.sqrt(h) * z\n",
    "        + 0.5 * sigma * s * sigma * ((tf.sqrt(h) * z) ** 2 - h)\n",
    "    )\n",
    "    return tf.add(idx, 1), s, sigma, mu, T, K\n",
    "\n",
    "# Monte Carlo loop\n",
    "def mc_body(idx, p):\n",
    "    _, _x, _sigma, _mu, _T, _K = tf.while_loop(\n",
    "        cond=lambda _idx, s, sigma, mu, T, K: _idx < N,\n",
    "        body=lambda _idx, s, sigma, mu, T, K: sde_body(\n",
    "            _idx, s, sigma, mu, T, K, mc_samples_ref, batch_size\n",
    "        ),\n",
    "        loop_vars=loop_var_mc,\n",
    "    )\n",
    "    return idx + 1, p + tf.reduce_mean(input_tensor=phi(_x, _sigma, _mu, _T, _K, 2), axis=0)\n",
    "\n",
    "    t0_train = time.time()\n",
    "    is_training = tf.compat.v1.placeholder(tf.bool, [])\n",
    "    u_approx = neural_net(xi, xi_approx, neurons, is_training, \"u_approx\", dtype=dtype)\n",
    "    loss = tf.reduce_mean(input_tensor=tf.math.squared_difference(u_approx, phi))\n",
    "\n",
    "    approx = tf.reduce_mean(input_tensor=u_approx)\n",
    "    reference = tf.reduce_mean(input_tensor=u_reference)\n",
    "    err = tf.abs(u_approx - u_reference)\n",
    "    err_l_1 = tf.reduce_mean(input_tensor=err)\n",
    "    err_l_2 = tf.reduce_mean(input_tensor=err**2)\n",
    "    err_l_inf = tf.reduce_max(input_tensor=err)\n",
    "    rel_err = err / tf.maximum(u_reference, 1e-4)\n",
    "    rel_err_l_1 = tf.reduce_mean(input_tensor=rel_err)\n",
    "    rel_err_l_2 = tf.reduce_mean(input_tensor=rel_err**2)\n",
    "    rel_err_l_inf = tf.reduce_max(input_tensor=rel_err)\n",
    "\n",
    "    lr = 0.01\n",
    "    step_rate = 40000\n",
    "    decay = 0.1\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    increment_global_step = tf.compat.v1.assign(global_step, global_step + 1)\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "        lr, global_step, step_rate, decay, staircase=True\n",
    "    )\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS, \"u_approx\")\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss, global_step)\n",
    "\n",
    "    file_out = open(file_name, \"w\")\n",
    "    file_out.write(\"step, li_err, learning_rate, time_train, time_test \\n \")\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        for step in range(train_steps):\n",
    "            if step % mc_freq == 0:\n",
    "                print(step)\n",
    "                t1_train = time.time()\n",
    "                _approximate_errors()\n",
    "                t0_train = time.time()\n",
    "            sess.run(train_op, feed_dict={is_training: True})\n",
    "        t1_train = time.time()\n",
    "        _approximate_errors()\n",
    "\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training parameter specification\n",
    "seed = 42\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    dtype = tf.float32\n",
    "\n",
    "    # Set network and training parameter\n",
    "    batch_size = 125000\n",
    "    batch_size_approx = batch_size\n",
    "    N, d = 128, 5\n",
    "    neurons = [50, 50, 1]\n",
    "    train_steps = 150000\n",
    "    mc_rounds, mc_freq = 100, 5000\n",
    "    mc_samples_ref, mc_rounds_ref = 1, 1\n",
    "    N_l = 1\n",
    "\n",
    "    # Define training and test interval\n",
    "    s_0_l = 80.0\n",
    "    s_0_r = 120.0\n",
    "    sigma_l = 0.1\n",
    "    sigma_r = 0.2\n",
    "    mu_l = 0.02\n",
    "    mu_r = 0.05\n",
    "    T_l = 0.9\n",
    "    T_r = 1.0\n",
    "    K_l = 109.0\n",
    "    K_r = 110.0\n",
    "    s_0_l_approx = 80.4\n",
    "    s_0_r_approx = 119.6\n",
    "    sigma_l_approx = 0.11\n",
    "    sigma_r_approx = 0.19\n",
    "    mu_l_approx = 0.03\n",
    "    mu_r_approx = 0.04\n",
    "    T_l_approx = 0.91\n",
    "    T_r_approx = 0.99\n",
    "    K_l_approx = 109.1\n",
    "    K_r_approx = 109.9\n",
    "    s0 = tf.random.uniform((batch_size, 1), minval=s_0_l, maxval=s_0_r, dtype=dtype)\n",
    "    sigma = tf.random.uniform(\n",
    "        (batch_size, 1), minval=sigma_l, maxval=sigma_r, dtype=dtype\n",
    "    )\n",
    "    mu = tf.random.uniform((batch_size, 1), minval=mu_l, maxval=mu_r, dtype=dtype)\n",
    "    T = tf.random.uniform((batch_size, 1), minval=T_l, maxval=T_r, dtype=dtype)\n",
    "    K = tf.random.uniform((batch_size, 1), minval=K_l, maxval=K_r, dtype=dtype)\n",
    "    s0_approx = tf.random.uniform(\n",
    "        (batch_size_approx, 1),\n",
    "        minval=s_0_l_approx,\n",
    "        maxval=s_0_r_approx,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    sigma_approx = tf.random.uniform(\n",
    "        (batch_size_approx, 1),\n",
    "        minval=sigma_l_approx,\n",
    "        maxval=sigma_r_approx,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    mu_approx = tf.random.uniform(\n",
    "        (batch_size_approx, 1), minval=mu_l_approx, maxval=mu_r_approx, dtype=dtype\n",
    "    )\n",
    "    T_approx = tf.random.uniform(\n",
    "        (batch_size_approx, 1), minval=T_l_approx, maxval=T_r_approx, dtype=dtype\n",
    "    )\n",
    "    K_approx = tf.random.uniform(\n",
    "        (batch_size_approx, 1), minval=K_l_approx, maxval=K_r_approx, dtype=dtype\n",
    "    )\n",
    "\n",
    "    xi = tf.reshape(tf.stack([s0, sigma, mu, T, K], axis=2), (batch_size, d))\n",
    "    xi_approx = tf.reshape(\n",
    "        tf.stack([s0_approx, sigma_approx, mu_approx, T_approx, K_approx], axis=2),\n",
    "        (batch_size_approx, d),\n",
    "    )\n",
    "\n",
    "    # Closed solution as reference\n",
    "    tfd = tfp.distributions\n",
    "    dist = tfd.Normal(loc=tf.cast(0.0, tf.float32), scale=tf.cast(1.0, tf.float32))\n",
    "    d1 = tf.math.divide(\n",
    "        (\n",
    "            tf.math.log(tf.math.divide(s0_approx, K_approx))\n",
    "            + (mu_approx + 0.5 * sigma_approx**2) * T_approx\n",
    "        ),\n",
    "        (sigma_approx * tf.sqrt(T_approx)),\n",
    "    )\n",
    "    d2 = tf.math.divide(\n",
    "        (\n",
    "            tf.math.log(tf.math.divide(s0_approx, K_approx))\n",
    "            + (mu_approx - 0.5 * sigma_approx**2) * T_approx\n",
    "        ),\n",
    "        (sigma_approx * tf.sqrt(T_approx)),\n",
    "    )\n",
    "    u_reference = tf.multiply(s0_approx, (dist.cdf(d1))) - K_approx * tf.exp(\n",
    "        -mu_approx * T_approx\n",
    "    ) * (dist.cdf(d2))\n",
    "    # European option\n",
    "    s0_v = tf.ones((mc_samples_ref, batch_size, 1), dtype) * s0\n",
    "    sigma_v = tf.ones((mc_samples_ref, batch_size, 1), dtype) * sigma\n",
    "    mu_v = tf.ones((mc_samples_ref, batch_size, 1), dtype) * mu\n",
    "    T_v = tf.ones((mc_samples_ref, batch_size, 1), dtype) * T\n",
    "    K_v = tf.ones((mc_samples_ref, batch_size, 1), dtype) * K\n",
    "    loop_var_mc = (\n",
    "        tf.constant(0),\n",
    "        s0_v,\n",
    "        sigma_v,\n",
    "        mu_v,\n",
    "        T_v,\n",
    "        K_v,\n",
    "    )\n",
    "    _, u = tf.while_loop(\n",
    "        cond=lambda idx, p: idx < N_l,\n",
    "        body=mc_body,\n",
    "        loop_vars=(tf.constant(0), tf.zeros((batch_size, 1), dtype)),\n",
    "    )\n",
    "    u_mc_test = u / tf.cast(N_l, tf.float32)\n",
    "\n",
    "    # Start training and testing\n",
    "    train_and_test(\n",
    "        xi,\n",
    "        xi_approx,\n",
    "        xi,\n",
    "        u_mc_test,\n",
    "        u_reference,\n",
    "        neurons,\n",
    "        train_steps,\n",
    "        mc_rounds,\n",
    "        mc_freq,\n",
    "        \"single-introductory.csv\",\n",
    "        dtype,\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
